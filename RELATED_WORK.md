# Related Work & Intellectual Foundation

This document maps the prior art honestly, identifies what is and isn't novel, and frames the contribution precisely.

## 1. Matryoshka Representation Learning

**Kusupati et al. (NeurIPS 2022)** introduced MRL: training a single encoder so every prefix of the embedding vector is independently useful. The loss is computed at multiple truncation points (e.g., dims 8, 16, 32, ..., 2048), forcing the model to front-load the most important semantic information into early dimensions. All prefixes serve the *same task* at different fidelities. The purpose is computational efficiency — use fewer dimensions when you need speed.

Key extensions:
- **Starbucks / 2D Matryoshka** (Li et al., 2024): extends MRL to both layer depth and embedding width
- **Matryoshka-Adaptor** (Yoon et al., EMNLP 2024): post-hoc adaptors for black-box embeddings
- **SMEC** (Zhang et al., EMNLP 2025): sequential MRL to reduce gradient variance during compression

**None of these use MRL for domain adaptation or assign different functional roles to different dimension ranges.**

## 2. Dimension-Level Functional Partitioning (the structural idea)

The idea of training different dimension ranges of a single embedding for different purposes predates our work:

- **Shi et al. (CVPR 2020)** — "Towards Universal Representation Learning for Deep Face Recognition." Split a single 512-dim face embedding into 16 contiguous sub-embeddings (32 dims each). Applied different losses (variation classification, variation adversarial) to different partitions. Some sub-embeddings became robust to specific nuisance factors (occlusion, pose), others handled identity. **This is the clearest structural precedent for dimension-level role assignment within a single vector.**

- **Browatzki et al. (ICCVW 2019)** — Split a single 99-dim face embedding into 4 contiguous sub-vectors for pose, identity, expression, and style. Supervised disentanglement on each.

- **Sanakoyeu et al. (CVPR 2019)** — "Divide and Conquer the Embedding Space for Metric Learning." Partition a single embedding layer into K groups of neurons, each with a separate metric learner. Different dimension groups get different supervision.

- **AN2VEC** (Lete et al., 2020) — Single GCN-VAE embedding with explicit dimension ranges for structure-only, feature-only, and shared information. Different reconstruction losses applied to different dimension slices.

- **F-Statistic Loss** (NeurIPS 2018) — Ensures distinct classes are well-separated on subsets of embedding dimensions, encouraging organic specialization. Subsets are learned, not pre-assigned.

## 3. Matryoshka + Functional Prefix Allocation

Very recent concurrent work has begun combining MRL's prefix structure with explicit functional roles:

- **TMRL** (arXiv:2601.05549, January 2026) — "Temporal-aware Matryoshka Representation Learning." Explicitly allocates the first *t* dimensions as a "temporal subspace" for time-related information, with remaining dimensions for general semantics. Uses a temporal projector to inject into the prefix, with dedicated temporal contrastive loss on the first *t* dims. **This is the closest structural precedent within the matryoshka framework specifically.**

- **DAME** (arXiv:2601.13999, January 2026) — "Duration-Aware Matryoshka Embedding." Different matryoshka prefix sizes are supervised with different data types (short vs. long utterances for speaker verification). Repurposes MRL nesting for duration-aware learning.

- **M-Vec** (arXiv:2409.15782, 2024) — Matryoshka speaker embeddings showing that even 16-dim prefixes retain speaker identity. Empirical evidence that identity concentrates in early dimensions.

## 4. Shared-Private Representation Decomposition

The goal of separating domain-invariant from domain-specific information is well-established, but traditionally uses **separate encoder branches**, not dimension partitioning within a single vector:

- **Domain Separation Networks** (Bousmalis et al., NeurIPS 2016) — Separate shared and private encoders with orthogonality constraints. The foundational shared-private architecture.

- **DIVA** (Ilse et al., PMLR 2020) — Three separate inference networks for domain, class, and residual latent subspaces.

- **DMVAE** (Lee & Pavlovic, CVPR Workshop 2021) — Latent vector structured as [private_1, shared, private_2], but generated by separate modality encoders.

- **SHAPED** (Zhang et al., NAACL 2018) — Shared/private at the parameter level for text style transfer.

- **DRANet** (Lee et al., CVPR 2021) — Single encoder, nonlinear decomposition into content and style via learned separator. Not dimension-range based.

## 5. Cross-Domain Identity and User Linking

- **Person Re-ID** literature (mostly vision) uses cross-camera/cross-modality matching but with separate encoder branches, not single-vector dimension partitioning
- **User Identity Linkage** across social networks (TransLink 2019, DeepMGGE 2020, various surveys) primarily uses graph/structural features, not text embeddings
- **Cross-domain recommendation** uses personality (Big Five) as domain-invariant transfer features (Zheng et al., 2020; ScienceDirect 2024), but for item recommendation, not identity matching
- **PAN shared tasks** (2019-2022) address cross-genre authorship verification, the closest existing benchmark to cross-domain text identity matching

## 6. Projection-Based Decomposition (alternative to dimension partitioning)

Several works decompose a single vector into shared/private components via mathematical projection rather than dimension slicing:

- **OE-CNN** (Wang et al., ECCV 2018) — Spherical coordinate decomposition: angular = identity, radial = age
- **DAL** (Wang et al., CVPR 2019) — Batch CCA factorization into identity-dependent and age-dependent components
- **GD-FAS** (Jung et al., ICCV 2025) — Gram-Schmidt orthogonalization into domain-invariant and domain-specific subvectors
- **Subspace Alignment** (Fernando et al., 2013) — PCA eigenvector alignment for domain adaptation

---

## Honest Novelty Assessment

### What is NOT novel (and we should not claim):
1. Training different losses on different dimension ranges of a single vector (Shi et al. 2020, Browatzki 2019, Sanakoyeu 2019)
2. Shared vs. private representation decomposition (DSN 2016, many others)
3. Allocating a matryoshka prefix for a specific functional purpose (TMRL, January 2026)
4. Matryoshka representation learning itself (Kusupati et al. 2022)

### What IS novel (our specific contribution):
1. **Cross-domain identity alignment via matryoshka prefix InfoNCE** — applying contrastive alignment specifically to the prefix dimensions across heterogeneous text domains. TMRL does prefix allocation for temporal info; we do it for domain-invariant identity. The application and loss design differ.
2. **Text-to-text cross-domain identity matching** — matching people across semantically heterogeneous text domains (dating ↔ hiring). No prior work addresses this task.
3. **Joint within-domain matryoshka + cross-domain prefix training in a single shared text encoder** — the combination of MRL-style multi-resolution within-domain loss with prefix-targeted cross-domain contrastive loss, using one encoder for both domains.

### How to frame the contribution:
> We combine dimension-level functional partitioning (established in face recognition: Shi et al. 2020) with matryoshka representation learning (Kusupati et al. 2022) and cross-domain contrastive alignment for a new task: text-based cross-domain identity matching. The prefix serves as a domain-invariant identity subspace trained with cross-domain InfoNCE, while the full matryoshka embedding preserves within-domain discriminative power.

---

## Key Citations (must-cite)

| Paper | Why |
|-------|-----|
| Kusupati et al., NeurIPS 2022 | MRL foundation |
| Shi et al., CVPR 2020 | Dimension-level functional partitioning in a single vector |
| Bousmalis et al., NeurIPS 2016 | Domain Separation Networks (shared-private concept) |
| TMRL, arXiv:2601.05549, Jan 2026 | Closest structural precedent — matryoshka prefix allocation |
| Sanakoyeu et al., CVPR 2019 | Divide and conquer embedding space |
| Ganin et al., JMLR 2016 | Domain-adversarial training (gradient reversal) |
| DAME, arXiv:2601.13999, Jan 2026 | Duration-aware matryoshka (concurrent) |

## Secondary Citations

| Paper | Why |
|-------|-----|
| F-Statistic Loss, NeurIPS 2018 | Dimension-subset disentanglement |
| Browatzki et al., ICCVW 2019 | Sub-vector role assignment |
| AN2VEC, 2020 | Explicit dimension partitioning in graph embeddings |
| SPCODEC, Interspeech 2025 | Dimension splitting in speech codecs |
| Chuang et al., 2020 | Theory: lower-dim embeddings improve domain invariance |
| PAN shared tasks, 2019-2022 | Cross-domain authorship verification benchmarks |
